{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e1c18a",
   "metadata": {},
   "source": [
    "# CNN with Distracted Driver Dataset Code\n",
    "This Jupyter Notebook will have a focus on using the distracted driver dataset to detect whether a driver is falling asleep or being distracted. This model will serve as a warning detection system for the coding firmware.\n",
    "\n",
    "### Using Keras\n",
    "Keras and Tensorflow are tools used to build and train machine learning models, especially deep learning models like neural networks. They are frameworks suited for deep learning tasks such as image recognition, natural language processing, and many more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ad217",
   "metadata": {},
   "source": [
    "## 1 | Importing Libraries\n",
    "Using a python installation, we import the following libraries / packages:\n",
    "- `numpy`: A library for numerical operations, providing support for large multi-dimensional arrays and matrices.\n",
    "- `pandas`: A data manipulation and analysis library that provides data structures like DataFrame for working with structured data.\n",
    "- `tensorflow`: An open-source framework for machine learning, enabling building and training of neural networks.\n",
    "- `os`: A module for interacting with the operating system, such as file and directory operations\n",
    "- `keras`: A high-level neural networks API, written in Python and capable of running on top of TensorFlow\n",
    "- `roboflow`: A package for accessing the Roboflow platform, which simplifies the process of training and deploying machine learning models\n",
    "- `sklearn`: Widely used open-source machine learning library for Python that provides simple and efficient tools for data mining and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c50787fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 10:30:46.436407: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-10 10:30:46.459636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731252646.483022 1111736 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731252646.490252 1111736 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-10 10:30:46.514877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# importing all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# importing keras & roboflow\n",
    "from tensorflow import keras\n",
    "from roboflow import Roboflow\n",
    "from PIL import Image\n",
    "\n",
    "# using sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# keras imports\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70ef5e",
   "metadata": {},
   "source": [
    "## 2 | Accessing Dataset via Roboflow\n",
    "Using Roboflow's API, we install our dataset of distracted driver images. We will be using this dataset to train our deep learning model and test it afterwards. Next, we use the validation is used to evaluate the deep learning model while its being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec50d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "rf = Roboflow(api_key=\"jFtnSW2KoFxmJxA2kF50\")\n",
    "project = rf.workspace(\"yolov8-z7kip\").project(\"distracted-driver-detection-bvtnl\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"multiclass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec14d1c",
   "metadata": {},
   "source": [
    "## 3 | Data Preprocessing\n",
    "Firstly, we start off by using pandas. With our newly installed dataset, we access the `.csv` files from the test & train and directory by converting it into a dataframe. We then access the distracted column and store it in a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb3ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('Distracted-Driver-Detection-2/test/_classes.csv')\n",
    "distracted_test_col = np.array(test_df[' Distracted'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c18c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Distracted-Driver-Detection-2/train/_classes.csv')\n",
    "distracted_train_col = np.array(train_df[' Distracted'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04844d18",
   "metadata": {},
   "source": [
    "### Grayscaling & Normalizing\n",
    "We design a function that takes in a array parameter. This function loops through the numpy arrays and checks whether it meets the requirements of the numpy array shape and check if the second index of the shape is 3.\n",
    "\n",
    "Once the requirements are satisfied, we grayscale the numpy array and then noramlize it from 0 to 1 by dividing by 255. The new numpy arrays are appended to the `grayscale` array and then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a7425bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(rgb_arrays):\n",
    "    grayscale = []\n",
    "    \n",
    "    for rgb_array in rgb_arrays:\n",
    "        if len(rgb_array.shape) == 3 and rgb_array.shape[2] == 3:\n",
    "            grayscale_image = 0.2989 * rgb_array[:, :, 0] + 0.5870 * rgb_array[:, :, 1] + 0.1140 * rgb_array[:, :, 2]\n",
    "            grayscale_image = grayscale_image / 255.0\n",
    "            grayscale.append(grayscale_image)\n",
    "        else:\n",
    "            print(\"Not in RGB format\")\n",
    "    \n",
    "    return np.array(grayscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab93bf",
   "metadata": {},
   "source": [
    "### Image Reading\n",
    "Next, we move on to reading the image files. We access the directory of `images` and then store it in the `directory_path` variable. We set up a empty array that will store numpy arrays in the variable `image_rgb_arrays`. We then read all the filenames in the `images` directory and then store them in the `filenames` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4b0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb_test_arrays = []\n",
    "directory_test_path = \"Distracted-Driver-Detection-2/test/images\"\n",
    "test_filenames = os.listdir(directory_test_path)\n",
    "\n",
    "image_rgb_train_arrays = []\n",
    "directory_train_path = \"Distracted-Driver-Detection-2/train/images\"\n",
    "train_filenames = os.listdir(directory_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e712c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in test_filenames:\n",
    "    image = Image.open(\"/home/chowdhuryj/madhacks/Distracted-Driver-Detection-2/test/images/\" + filename)\n",
    "    image_array = np.array(image)\n",
    "    image_rgb_test_arrays.append(image_array)\n",
    "    \n",
    "for filename in train_filenames:\n",
    "    image = Image.open(\"/home/chowdhuryj/madhacks/Distracted-Driver-Detection-2/train/images/\" + filename)\n",
    "    image_array = np.array(image)\n",
    "    image_rgb_train_arrays.append(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886e842",
   "metadata": {},
   "source": [
    "### Storing the New Numpy Arrays\n",
    "Lastly, we store the grayscaled and noramlized numpy arrays in the `grayscale_normalized_images` variable. Next, we perform the following steps detailed below:\n",
    "1. We rehape the numpy arrays to match the shape required by the CNN model so that we can prepare it for training the model\n",
    "2. We then convert the data type of the numpy arrays to `float32` as it uses less memory and makes memory usage manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e398d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_normalized_test_images = np.array(grayscale(image_rgb_test_arrays))\n",
    "grayscale_normalized_train_images = np.array(grayscale(image_rgb_train_arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c127ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_normalized_train_images = grayscale_normalized_train_images.reshape(-1, 640, 640, 1)\n",
    "grayscale_normalized_test_images = grayscale_normalized_test_images.reshape(-1, 640, 640, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7761b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_normalized_train_images = grayscale_normalized_train_images.astype(\"float32\")\n",
    "grayscale_normalized_test_images = grayscale_normalized_test_images.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75249b62",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "The final step of our data pre-processing is data augmentation. We end by performing data augmentation using `ImageDataGenerator`, which generates new, varied versions of the training images in each epoch, which helps the model recognize patterns more effectively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4e6021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ImageDataGenerator with augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,             # Random rotations (degrees)\n",
    "    width_shift_range=0.2,         # Random horizontal shifts\n",
    "    height_shift_range=0.2,        # Random vertical shifts\n",
    "    shear_range=0.2,               # Random shear transformations\n",
    "    zoom_range=0.2,                # Random zoom\n",
    "    horizontal_flip=True,          # Randomly flip images horizontally\n",
    "    fill_mode='nearest'            # Strategy for filling missing pixels after transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc63572",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(grayscale_normalized_train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a9e613",
   "metadata": {},
   "source": [
    "## 4 | Building the Convolutional Neural Network\n",
    "Great! We're done with data pre-processing! Now, we move on to building our Convolutional Neural Network! We start off by initializing a Sequential model, which allows us to stack layers in order. We used a input later, convolutional, max pooling, flatten and dense layers as described in detail below:\n",
    "\n",
    "### Input, Convolutional & Max Pooling Layers\n",
    "1. `Input Layer`: the input layer expects grayscale images which are 640 x 640\n",
    "2. `First Convolutional Layer`: this layer applies 64 filters, each of the size 3 x 3 with a `ReLU` activation function\n",
    "3. `First Max Pooling Layer`: this layer downsamples the feature maps, reducing the dimensions by taking the maximum value in each 3x3 window\n",
    "4. `Second Convolutional Layer`: this layer applies 32 filters of the size `3x3` and uses `ReLU` for non-linearity\n",
    "5. `Second Max Pooling Layer`: this layer downsamples the features maps, reducing data size and focuses on key patterns\n",
    "6. `Flatten Layer`: this layer converts the 2D feature maps into a 1D array, preparing the data for the dense layers.\n",
    "\n",
    "### Dense & Output Layers\n",
    "7. `First Dense Layer:` this is a dense layer with 128 neurons and ReLU activation, which learns high-level patterns\n",
    "8. `Second Dense Layer`: this is a dense layer with 64 neurons, learning more abstracted patterns\n",
    "9. `Third Dense Layer`: this is a dense layer with 32 neurons, which further abstracts the features\n",
    "10. `Ouput Layer`: this is the final layer  with 1 neuron and sigmoid activation function, which is ideal for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fdc4e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731252665.629768 1111736 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:60:00.0, compute capability: 7.5\n",
      "I0000 00:00:1731252665.640203 1111736 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13764 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:61:00.0, compute capability: 7.5\n",
      "I0000 00:00:1731252665.649339 1111736 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 13764 MB memory:  -> device: 2, name: Tesla T4, pci bus id: 0000:da:00.0, compute capability: 7.5\n",
      "I0000 00:00:1731252665.652124 1111736 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 13764 MB memory:  -> device: 3, name: Tesla T4, pci bus id: 0000:db:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# creating a model\n",
    "model = Sequential()\n",
    "\n",
    "# add input layer\n",
    "model.add(Input(shape=(640, 640, 1)))\n",
    "\n",
    "# adding the model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa84bcb",
   "metadata": {},
   "source": [
    "## 5 | Compliling the Model\n",
    "Next, we compile the model, specify the optimization algorithm, the loss function and the metric for evaluating the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3274b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8a1a1",
   "metadata": {},
   "source": [
    "## 6 | Training & Saving the Model\n",
    "Lastly, we compile the model using the `fit()`, specifying the training data, validation data and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517f556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731252669.833707 1112955 service.cc:148] XLA service 0x7fb36c0048f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731252669.833737 1112955 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1731252669.833740 1112955 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1731252669.833742 1112955 service.cc:156]   StreamExecutor device (2): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1731252669.833744 1112955 service.cc:156]   StreamExecutor device (3): Tesla T4, Compute Capability 7.5\n",
      "2024-11-10 10:31:09.881496: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1731252670.121475 1112955 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-10 10:31:12.016009: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[32,64,638,638]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,1,640,640]{3,2,1,0}, f32[64,1,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2024-11-10 10:31:12.664452: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[32,32,210,210]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,212,212]{3,2,1,0}, f32[32,64,3,3]{3,2,1,0}, f32[32]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2024-11-10 10:31:20.194206: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[32,64,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,212,212]{3,2,1,0}, f32[32,32,210,210]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2024-11-10 10:31:20.783382: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.589355974s\n",
      "Trying algorithm eng0{} for conv (f32[32,64,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,212,212]{3,2,1,0}, f32[32,32,210,210]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "I0000 00:00:1731252681.985009 1112955 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.6388 - loss: 1.4453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 10:31:30.216794: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[18,64,638,638]{3,2,1,0}, u8[0]{0}) custom-call(f32[18,1,640,640]{3,2,1,0}, f32[64,1,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2024-11-10 10:31:30.773075: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[18,32,210,210]{3,2,1,0}, u8[0]{0}) custom-call(f32[18,64,212,212]{3,2,1,0}, f32[32,64,3,3]{3,2,1,0}, f32[32]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564ms/step - accuracy: 0.6392 - loss: 1.4270"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 10:31:39.767773: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[32,64,638,638]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,1,640,640]{3,2,1,0}, f32[64,1,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2024-11-10 10:31:40.427898: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[32,32,210,210]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,212,212]{3,2,1,0}, f32[32,64,3,3]{3,2,1,0}, f32[32]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2024-11-10 10:31:43.973179: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[28,64,638,638]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,1,640,640]{3,2,1,0}, f32[64,1,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2024-11-10 10:31:44.560021: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[28,32,210,210]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,64,212,212]{3,2,1,0}, f32[32,64,3,3]{3,2,1,0}, f32[32]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 898ms/step - accuracy: 0.6396 - loss: 1.4099 - val_accuracy: 0.6859 - val_loss: 0.6521\n",
      "Epoch 2/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 260ms/step - accuracy: 0.6406 - loss: 0.6595 - val_accuracy: 0.6859 - val_loss: 0.6351\n",
      "Epoch 3/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 261ms/step - accuracy: 0.6643 - loss: 0.6399 - val_accuracy: 0.6859 - val_loss: 0.6353\n",
      "Epoch 4/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 261ms/step - accuracy: 0.6513 - loss: 0.6314 - val_accuracy: 0.6731 - val_loss: 0.6364\n",
      "Epoch 5/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 263ms/step - accuracy: 0.6840 - loss: 0.6030 - val_accuracy: 0.5833 - val_loss: 0.6575\n",
      "Epoch 6/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 264ms/step - accuracy: 0.7339 - loss: 0.5473 - val_accuracy: 0.6474 - val_loss: 0.7024\n",
      "Epoch 7/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 263ms/step - accuracy: 0.8204 - loss: 0.4252 - val_accuracy: 0.6090 - val_loss: 0.7207\n",
      "Epoch 8/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 263ms/step - accuracy: 0.8682 - loss: 0.3370 - val_accuracy: 0.5962 - val_loss: 0.8625\n",
      "Epoch 9/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 263ms/step - accuracy: 0.9196 - loss: 0.2131 - val_accuracy: 0.6026 - val_loss: 1.0381\n",
      "Epoch 10/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 266ms/step - accuracy: 0.9470 - loss: 0.1910 - val_accuracy: 0.5962 - val_loss: 1.1836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fb4c802a400>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(grayscale_normalized_train_images, distracted_train_col,\n",
    "          validation_data=(grayscale_normalized_test_images, distracted_test_col), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df12d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('distracted_driver_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madhacks",
   "language": "python",
   "name": "madhacks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
